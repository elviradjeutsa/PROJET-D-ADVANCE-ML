{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d50687-63d2-40c7-835f-0e1601f37cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8be779f3-0745-4dc9-8dea-f264f9147b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b7f24-d000-4989-8d66-4cffd6404c61",
   "metadata": {},
   "source": [
    "Here we instantiate an LSTM cell. W is the weight matrix that applies to the current input, U applies to the hidden state coming from the previous time period and b is the bias matrix. \n",
    "\n",
    "We then initialize the parameters. Biases can be initialized to zero but not the weight matrixes. Otherwise the derivative would be equal to zero and we could not update the weights. Here, we use the Xavier initialization which takes into account the size of the input and output to adjust the standard deviation of the normal distribution used to initialize the values. The mean of this distribution is, of course zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "77751ee8-3683-449f-b872-1b332ed8b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Input gate parameters\n",
    "        self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.U_i = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Forget gate parameters\n",
    "        self.W_f = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.U_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Cell gate parameters\n",
    "        self.W_c = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.U_c = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Output gate parameters\n",
    "        self.W_o = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.U_o = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"W_\" in name or \"U_\" in name:  \n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif \"b_\" in name:  \n",
    "                nn.init.zeros_(param)\n",
    "                \n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(x.size(1)):\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            # Input gate\n",
    "            i_t = torch.sigmoid(x_t @ self.W_i + h_prev @ self.U_i + self.b_i)\n",
    "\n",
    "            # Forget gate\n",
    "            f_t = torch.sigmoid(x_t @ self.W_f + h_prev @ self.U_f + self.b_f)\n",
    "\n",
    "            # Cell gate\n",
    "            g_t = torch.tanh(x_t @ self.W_c + h_prev @ self.U_c + self.b_c)\n",
    "\n",
    "            # Update cell state\n",
    "            c_t = f_t * c_prev + i_t * g_t\n",
    "\n",
    "            # Output gate\n",
    "            o_t = torch.sigmoid(x_t @ self.W_o + h_prev @ self.U_o + self.b_o)\n",
    "\n",
    "            # Update hidden state\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            outputs.append(h_t.unsqueeze(1))\n",
    "\n",
    "            # Update states for next time step\n",
    "            h_prev, c_prev = h_t, c_t\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e72b7f-3eaf-44b8-a309-f82bc394a1ff",
   "metadata": {},
   "source": [
    "Here, we create the LSTM layers. The for loop in the initialization stacks the cells to create the layers. In the end, we add a linear layer to convert the last hidden state into the output size we desire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b0c92bc2-6cd2-461d-a9bc-4bd75355b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, output_size=1):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_size = input_size if i == 0 else hidden_size\n",
    "            self.layers.append(LSTM(in_size, hidden_size))\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_states):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h_prev, c_prev = hidden_states[i]\n",
    "            x, (h, c) = layer(x, (h_prev, c_prev))  # Pass through each LSTM layer\n",
    "\n",
    "        # Use the last time step's hidden state and apply the fully connected layer\n",
    "        outputs = self.fc(x[:, -1, :])  # Shape: (batch_size, output_size)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "96bc95da-5686-4c90-9ed9-0c20d1f57377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_897/3479382016.py:1: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_merged = pd.read_csv('data_merged.csv')\n"
     ]
    }
   ],
   "source": [
    "data_merged = pd.read_csv('data_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825a0fe-ca6a-40a6-8995-264e41c1d699",
   "metadata": {},
   "source": [
    "For the training, we only keep the client present during the three years (400 000 people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "882e0316-9076-4c66-bc3f-053738e7042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_data = data_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e9dce956-d1ff-4ca7-817f-02ad3ee65f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data = non_nan_data.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2ffc0d9b-d1ab-4ad0-8ecd-4711174ecac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of zeros: 87.8%\n"
     ]
    }
   ],
   "source": [
    "zero_count = (subset_data['fra_1123'] == 0).sum()\n",
    "total_count = len(subset_data['fra_0123'])\n",
    "percentage_of_zeros = (zero_count / total_count) * 100\n",
    "\n",
    "print(f\"Percentage of zeros: {percentage_of_zeros}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de19bc79-bef6-48a7-bb5c-a35927d060eb",
   "metadata": {},
   "source": [
    "We now need to reshape the data so that it can be fed into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c059b34f-ee1b-4258-ab6f-efef5b4092c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static Data Shape: (1000, 4)\n",
      "Time Data Shape: (1000, 31)\n"
     ]
    }
   ],
   "source": [
    "# Extract time-dependent columns (these will be the targets)\n",
    "time_columns = [col for col in subset_data.columns if col.startswith('fra_')]\n",
    "\n",
    "# Extract static columns (these will be the features)\n",
    "static_columns = ['Age22', 'category_Enfant', 'category_Homme', 'category_Femme']  # Static features\n",
    "\n",
    "# Create the static feature matrix\n",
    "static_data = subset_data[static_columns].values  # Shape: (num_clients, num_features)\n",
    "\n",
    "# Create the time-dependent target matrix\n",
    "time_data = subset_data[time_columns].values  # Shape: (num_clients, num_time_steps)\n",
    "\n",
    "print(\"Static Data Shape:\", static_data.shape)  # (num_clients, num_features)\n",
    "print(\"Time Data Shape:\", time_data.shape)  # (num_clients, num_time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce0cd7-16c7-4840-b78d-f38f7543f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([7000, 24, 4])\n",
      "Target Shape: torch.Size([7000, 1])\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "sequence_length = 24  # Length of each input sequence (e.g., one year)\n",
    "\n",
    "# Iterate over clients\n",
    "for client_idx in range(static_data.shape[0]):  # Iterate over each client\n",
    "    client_targets = time_data[client_idx]  # Shape: (num_time_steps,)\n",
    "\n",
    "    # Create sequences for this client\n",
    "    for t in range(len(client_targets) - sequence_length):  # Slide over time steps\n",
    "        # Static features repeated for each time step in the sequence\n",
    "        static_features = static_data[client_idx]  # Shape: (num_features,)\n",
    "        static_repeated = np.tile(static_features, (sequence_length, 1))  # Shape: (seq_len, num_features)\n",
    "    \n",
    "        # Ensure the static features are of type float32 (numeric)\n",
    "        static_repeated = static_repeated.astype(np.float32)\n",
    "    \n",
    "        # Combine static and time-dependent features\n",
    "        X.append(static_repeated)  # Static features remain constant for each sequence\n",
    "        y.append(client_targets[t + sequence_length])  # Target is the value at the next time step\n",
    "        # y.append(torch.ones(1).unsqueeze(0))\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)  # Shape: (num_samples, seq_len, num_features)\n",
    "y = np.array(y)  # Shape: (num_samples,)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)  # Shape: (num_samples, seq_len, input_size)\n",
    "y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Shape: (num_samples, 1)\n",
    "\n",
    "print(\"Input Shape:\", X.shape)  # (num_samples, seq_len, input_size)\n",
    "print(\"Target Shape:\", y.shape)  # (num_samples, 1)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = static_data.shape[1]  # Number of static features\n",
    "hidden_size = 32  # Size of LSTM hidden states\n",
    "num_layers = 5    # Number of LSTM layers\n",
    "output_size = 1   # Predicting a single value (next step's target)\n",
    "batch_size = 4    # Number of samples per batch\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = StackedLSTM(input_size, hidden_size, num_layers=5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize hidden and cell states\n",
    "def init_hidden_states(batch_size, num_layers, hidden_size, device):\n",
    "    return [(torch.zeros(batch_size, hidden_size).to(device), torch.zeros(batch_size, hidden_size).to(device)) \n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    all_predictions = []\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        batch_X, batch_y = batch_X.to(torch.float32), batch_y.to(torch.float32)\n",
    "\n",
    "        # Initialize hidden states for the batch\n",
    "        hidden_states = init_hidden_states(batch_size, num_layers=5, hidden_size=hidden_size, device=batch_X.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X, hidden_states)\n",
    "        all_predictions.append(outputs.detach().cpu().numpy())\n",
    "        loss = criterion(outputs, batch_y)  # Direct comparison with targets\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # After the loop, concatenate all predictions\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "    # Calculate statistics\n",
    "    avg_prediction = np.mean(all_predictions)\n",
    "    min_prediction = np.min(all_predictions)\n",
    "    max_prediction = np.max(all_predictions)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "    print(f\"Predictions - Average: {avg_prediction:.4f}, Min: {min_prediction:.4f}, Max: {max_prediction:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0664bf95-0043-4c66-a156-5e7e6cebc972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entite_eco',\n",
       " 'personne_morale',\n",
       " 'annee_soins',\n",
       " 'colloc',\n",
       " 'adh_fac',\n",
       " 'type_cont',\n",
       " 'code_postal',\n",
       " 'Age22',\n",
       " 'tranche_age22',\n",
       " 'rg_benef',\n",
       " 'PRES2201',\n",
       " 'PRES2202',\n",
       " 'PRES2203',\n",
       " 'PRES2204',\n",
       " 'PRES2205',\n",
       " 'PRES2206',\n",
       " 'PRES2207',\n",
       " 'PRES2208',\n",
       " 'PRES2209',\n",
       " 'PRES2210',\n",
       " 'PRES2211',\n",
       " 'PRES2212',\n",
       " 'nb_occ_id',\n",
       " 'ra_0122',\n",
       " 'ra_0222',\n",
       " 'ra_0322',\n",
       " 'ra_0422',\n",
       " 'ra_0522',\n",
       " 'ra_0622',\n",
       " 'ra_0722',\n",
       " 'ra_0822',\n",
       " 'ra_0922',\n",
       " 'ra_1022',\n",
       " 'ra_1122',\n",
       " 'ra_1222',\n",
       " 'fra_0122',\n",
       " 'fra_0222',\n",
       " 'fra_0322',\n",
       " 'fra_0422',\n",
       " 'fra_0522',\n",
       " 'fra_0622',\n",
       " 'fra_0722',\n",
       " 'fra_0822',\n",
       " 'fra_0922',\n",
       " 'fra_1022',\n",
       " 'fra_1122',\n",
       " 'fra_1222',\n",
       " 'delta_0122',\n",
       " 'delta_0222',\n",
       " 'delta_0322',\n",
       " 'delta_0422',\n",
       " 'delta_0522',\n",
       " 'delta_0622',\n",
       " 'delta_0722',\n",
       " 'delta_0822',\n",
       " 'delta_0922',\n",
       " 'delta_1022',\n",
       " 'delta_1122',\n",
       " 'delta_1222',\n",
       " 'PRES2301',\n",
       " 'PRES2302',\n",
       " 'PRES2303',\n",
       " 'PRES2304',\n",
       " 'PRES2305',\n",
       " 'PRES2306',\n",
       " 'PRES2307',\n",
       " 'PRES2308',\n",
       " 'PRES2309',\n",
       " 'PRES2310',\n",
       " 'PRES2311',\n",
       " 'PRES2312',\n",
       " 'ra_0123',\n",
       " 'ra_0223',\n",
       " 'ra_0323',\n",
       " 'ra_0423',\n",
       " 'ra_0523',\n",
       " 'ra_0623',\n",
       " 'ra_0723',\n",
       " 'ra_0823',\n",
       " 'ra_0923',\n",
       " 'ra_1023',\n",
       " 'ra_1123',\n",
       " 'ra_1223',\n",
       " 'fra_0123',\n",
       " 'fra_0223',\n",
       " 'fra_0323',\n",
       " 'fra_0423',\n",
       " 'fra_0523',\n",
       " 'fra_0623',\n",
       " 'fra_0723',\n",
       " 'fra_0823',\n",
       " 'fra_0923',\n",
       " 'fra_1023',\n",
       " 'fra_1123',\n",
       " 'fra_1223',\n",
       " 'delta_0123',\n",
       " 'delta_0223',\n",
       " 'delta_0323',\n",
       " 'delta_0423',\n",
       " 'delta_0523',\n",
       " 'delta_0623',\n",
       " 'delta_0723',\n",
       " 'delta_0823',\n",
       " 'delta_0923',\n",
       " 'delta_1023',\n",
       " 'delta_1123',\n",
       " 'delta_1223',\n",
       " 'PRES2401',\n",
       " 'PRES2402',\n",
       " 'PRES2403',\n",
       " 'PRES2404',\n",
       " 'PRES2405',\n",
       " 'PRES2406',\n",
       " 'PRES2407',\n",
       " 'PRES2408',\n",
       " 'PRES2409',\n",
       " 'PRES2410',\n",
       " 'PRES2411',\n",
       " 'PRES2412',\n",
       " 'ra_0124',\n",
       " 'ra_0224',\n",
       " 'ra_0324',\n",
       " 'ra_0424',\n",
       " 'ra_0524',\n",
       " 'ra_0624',\n",
       " 'ra_0724',\n",
       " 'ra_0824',\n",
       " 'fra_0224',\n",
       " 'fra_0324',\n",
       " 'fra_0424',\n",
       " 'fra_0524',\n",
       " 'fra_0624',\n",
       " 'fra_0724',\n",
       " 'fra_0824',\n",
       " 'delta_0124',\n",
       " 'delta_0224',\n",
       " 'delta_0324',\n",
       " 'delta_0424',\n",
       " 'delta_0524',\n",
       " 'delta_0624',\n",
       " 'delta_0724',\n",
       " 'delta_0824',\n",
       " 'category_Autre',\n",
       " 'category_Conjoint',\n",
       " 'category_Enfant majeur',\n",
       " 'category_Enfant mineur',\n",
       " 'category_Salarié',\n",
       " 'category_Enfant',\n",
       " 'category_Femme',\n",
       " 'category_Homme',\n",
       " 'category_Pas de donnees',\n",
       " 'category_ND',\n",
       " 'category_Salarié et 1 enfant',\n",
       " 'category_Salarié et 2 enfants',\n",
       " 'category_Salarié et 3+ enfants',\n",
       " 'category_Salarié et conjoint et 1 enfant',\n",
       " 'category_Salarié et conjoint et 2 enfants',\n",
       " 'category_Salarié et conjoint et 3+ enfants',\n",
       " 'category_Salarié et conjoint sans enfant',\n",
       " 'category_Salarié seul']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(subset_data.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
